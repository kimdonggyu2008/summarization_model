{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rjs5AikBrzgP",
        "zHH3bg8NmDRO",
        "FtPuTEaYlgXG",
        "XmVm2idZl3-N",
        "dQE0_2KUl57l",
        "fPAytphkl0Wd",
        "k3Q_SiWslyW1",
        "6SOgetzulwfl",
        "1fkStaC0lt0G",
        "gps_AH-hlqbm",
        "Cf_Sj0SxmIzM",
        "9zi0dZMQmK3N",
        "rNPobn_NmnXk",
        "wgbX0kuJmQxV"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#사전세팅\n"
      ],
      "metadata": {
        "id": "rjs5AikBrzgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sap7tMgiSV7",
        "outputId": "1ea3747b-d185-4bc1-9aa5-79f70865c3c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45VxEHKOiUQC",
        "outputId": "050dca82-223f-4992-d961-901828cd3c45"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"#내용\n",
        "\n",
        "    This is the configuration class to store the configuration of a [`PegasusXModel`]. It is used to instantiate a\n",
        "    PEGASUS-X model according to the specified arguments, defining the model architecture. Instantiating a\n",
        "    configuration with the defaults will yield a similar configuration to that of the PEGASUS-X\n",
        "    [google/pegasus-x-large](https://huggingface.co/google/pegasus-x-large) architecture.\n",
        "\n",
        "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
        "    documentation from [`PretrainedConfig`] for more information.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        vocab_size (`int`, *optional*, defaults to 96103):\n",
        "            Vocabulary size of the PEGASUS-X model. Defines the number of different tokens that can be represented by\n",
        "            the `inputs_ids` passed when calling [`PegasusXModel`].\n",
        "        d_model (`int`, *optional*, defaults to 1024):\n",
        "            Dimension of the layers and the pooler layer.\n",
        "        encoder_layers (`int`, *optional*, defaults to 16):\n",
        "            Number of encoder layers.\n",
        "        decoder_layers (`int`, *optional*, defaults to 16):\n",
        "            Number of decoder layers.\n",
        "        encoder_attention_heads (`int`, *optional*, defaults to 16):\n",
        "            Number of attention heads for each attention layer in the Transformer encoder.\n",
        "        decoder_attention_heads (`int`, *optional*, defaults to 16):\n",
        "            Number of attention heads for each attention layer in the Transformer decoder.\n",
        "        decoder_ffn_dim (`int`, *optional*, defaults to 4096):\n",
        "            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n",
        "        encoder_ffn_dim (`int`, *optional*, defaults to 4096):\n",
        "            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n",
        "        activation_function (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n",
        "            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n",
        "            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n",
        "        dropout (`float`, *optional*, defaults to 0.1):\n",
        "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
        "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
        "            The dropout ratio for the attention probabilities.\n",
        "        activation_dropout (`float`, *optional*, defaults to 0.0):\n",
        "            The dropout ratio for activations inside the fully connected layer.\n",
        "        max_position_embeddings (`int`, *optional*, defaults to 16384):\n",
        "            The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
        "            just in case (e.g., 512 or 1024 or 2048).\n",
        "        init_std (`float`, *optional*, defaults to 0.02):\n",
        "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
        "        encoder_layerdrop (`float`, *optional*, defaults to 0.0):\n",
        "            The LayerDrop probability for the encoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)\n",
        "            for more details.\n",
        "        decoder_layerdrop (`float`, *optional*, defaults to 0.0):\n",
        "            The LayerDrop probability for the decoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)\n",
        "            for more details.\n",
        "        use_cache (`bool`, *optional*, defaults to `True`):\n",
        "            Whether or not the model should return the last key/values attentions (not used by all models)\n",
        "        forced_eos_token_id (`int`, *optional*, defaults to 1):\n",
        "            The id of the token to force as the last generated token when `max_length` is reached. Usually set to\n",
        "            `eos_token_id`.\n",
        "        num_global_tokens (`int`, *optional*, defaults to 128):\n",
        "            Number of global tokens to use for the encoder\n",
        "        block_size (`int`, *optional*, defaults to 512):\n",
        "            Block size for encoder local attention. Sequence length should be an exact multiple of block size.\n",
        "            block_size must be a multiple of 2 if stagger_local_block is True\n",
        "        stagger_local_block (`bool`, *optional*, defaults to `True`):\n",
        "            Whether to stagger every other local attention by half a block\n",
        "\n",
        "    Example:\n",
        "\n",
        "    ```python\n",
        "    >>> from transformers import PegasusXConfig, PegasusXModel\n",
        "\n",
        "    >>> # Initializing a PEGASUS google/pegasus-x-large style configuration\n",
        "    >>> configuration = PegasusXConfig()\n",
        "\n",
        "    >>> # Initializing a model (with random weights) from the google/pegasus-x-large style configuration\n",
        "    >>> model = PegasusXModel(configuration)\n",
        "\n",
        "    >>> # Accessing the model configuration\n",
        "    >>> configuration = model.config\n",
        "    ```\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pcdmwcJPsAMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#사전 설정"
      ],
      "metadata": {
        "id": "zHH3bg8NmDRO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VwyY7ZxocfSo"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import math\n",
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutput,\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput,\n",
        ")\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.utils import (\n",
        "    add_end_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    logging,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from transformers.models.pegasus.configuration_pegasus import PegasusConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"google/pegasus-x-base\"\n",
        "_CONFIG_FOR_DOC = \"PegasusXConfig\""
      ],
      "metadata": {
        "id": "NzgQhYHphri6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#config(실행조건)\n"
      ],
      "metadata": {
        "id": "FtPuTEaYlgXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"PEGASUS-X model configuration\"\"\"\n",
        "\n",
        "from transformers import PretrainedConfig\n",
        "from transformers.utils import logging\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "class PegasusXConfig(PretrainedConfig):\n",
        "\n",
        "    model_type = \"pegasus_x\"\n",
        "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
        "    attribute_map = {\"num_attention_heads\": \"encoder_attention_heads\", \"hidden_size\": \"d_model\"}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=96103,\n",
        "        max_position_embeddings=16384,\n",
        "        encoder_layers=16,\n",
        "        encoder_ffn_dim=4096,\n",
        "        encoder_attention_heads=16,\n",
        "        decoder_layers=16,\n",
        "        decoder_ffn_dim=4096,\n",
        "        decoder_attention_heads=16,\n",
        "        encoder_layerdrop=0.0,\n",
        "        decoder_layerdrop=0.0,\n",
        "        use_cache=True,\n",
        "        is_encoder_decoder=True,\n",
        "        activation_function=\"gelu\",\n",
        "        d_model=1024,\n",
        "        dropout=0.1,\n",
        "        attention_dropout=0.0,\n",
        "        activation_dropout=0.0,\n",
        "        init_std=0.02,\n",
        "        decoder_start_token_id=0,\n",
        "        scale_embedding=True,\n",
        "        pad_token_id=0,\n",
        "        eos_token_id=1,\n",
        "        forced_eos_token_id=1,\n",
        "        num_global_tokens=32,\n",
        "        block_size=512,\n",
        "        stagger_local_blocks=True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.d_model = d_model\n",
        "        self.encoder_ffn_dim = encoder_ffn_dim\n",
        "        self.encoder_layers = encoder_layers\n",
        "        self.encoder_attention_heads = encoder_attention_heads\n",
        "        self.decoder_ffn_dim = decoder_ffn_dim\n",
        "        self.decoder_layers = decoder_layers\n",
        "        self.decoder_attention_heads = decoder_attention_heads\n",
        "        self.dropout = dropout\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.activation_dropout = activation_dropout\n",
        "        self.activation_function = activation_function\n",
        "        self.init_std = init_std\n",
        "        self.encoder_layerdrop = encoder_layerdrop\n",
        "        self.decoder_layerdrop = decoder_layerdrop\n",
        "        self.use_cache = use_cache\n",
        "        self.num_hidden_layers = encoder_layers\n",
        "        self.scale_embedding = scale_embedding  # scale factor will be sqrt(d_model) if True\n",
        "\n",
        "        self.num_global_tokens = num_global_tokens\n",
        "        self.block_size = block_size\n",
        "        self.stagger_local_blocks = stagger_local_blocks\n",
        "\n",
        "        super().__init__(\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            is_encoder_decoder=is_encoder_decoder,\n",
        "            decoder_start_token_id=decoder_start_token_id,\n",
        "            forced_eos_token_id=forced_eos_token_id,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def num_attention_heads(self) -> int:\n",
        "        return self.encoder_attention_heads\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self) -> int:\n",
        "        return self.d_model"
      ],
      "metadata": {
        "id": "ZZA-6Tt9kUQA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#내부 구조 선언"
      ],
      "metadata": {
        "id": "XmVm2idZl3-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclasses.dataclass\n",
        "class DimensionInfo:\n",
        "    \"\"\"Wrapper for dimension info.\"\"\"\n",
        "\n",
        "    batch_size:8192   # batch size\n",
        "    seq_len: 2048  # token length\n",
        "    block_size: 2048  # block size\n",
        "    num_heads: 16  # num heads\n",
        "    hidden_dim: 1024  # hidden dim\n",
        "    dim_per_head: 64  # dim per head\n",
        "    num_blocks: 16  # num blocks\n",
        "    global_len: 16  # global length\n",
        "    padded_seq_len: 2048  # padded token seq length\n",
        "\n",
        "    # Note: Compared to the original Flax implementation, we will pad the token representations to\n",
        "    #       a multiple of block size at the start of the encoder layers, so T=P always."
      ],
      "metadata": {
        "id": "O4w3tULGho0q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#좌우 반전?"
      ],
      "metadata": {
        "id": "dQE0_2KUl57l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Copied from transformers.models.bart.modeling_bart.shift_tokens_right\n",
        "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
        "    \"\"\"\n",
        "    Shift input ids one token to the right.\n",
        "    \"\"\"\n",
        "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "    if pad_token_id is None:\n",
        "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
        "    # replace possible -100 values in labels by `pad_token_id`\n",
        "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "\n",
        "    return shifted_input_ids\n",
        "\n"
      ],
      "metadata": {
        "id": "ANVmF7CdhnkD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#워드 임베딩"
      ],
      "metadata": {
        "id": "fPAytphkl0Wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->PegasusX\n",
        "class PegasusXScaledWordEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    This module overrides nn.Embeddings' forward by multiplying with embeddings scale.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float] = 1.0):\n",
        "        super().__init__(num_embeddings, embedding_dim, padding_idx)\n",
        "        self.embed_scale = embed_scale\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor):\n",
        "        return super().forward(input_ids) * self.embed_scale\n"
      ],
      "metadata": {
        "id": "gWnTzMMnhiOT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#포지셔널 임베딩"
      ],
      "metadata": {
        "id": "k3Q_SiWslyW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class PegasusXSinusoidalPositionalEmbedding(nn.Module):\n",
        "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, max_scale: int = 10000.0):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_scale = max_scale\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, input_embeds: torch.Tensor, past_key_values_length: int = 0) -> torch.Tensor:\n",
        "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
        "        batch_size, seq_len = input_embeds.shape[:2]\n",
        "        positions = torch.arange(\n",
        "            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=input_embeds.device\n",
        "        )[:, None]\n",
        "        pe = torch.zeros((seq_len, self.embed_dim), device=input_embeds.device, dtype=input_embeds.dtype)\n",
        "        half_d_feature = self.embed_dim // 2\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(half_d_feature, device=input_embeds.device, dtype=torch.int64).type_as(input_embeds)\n",
        "            * -(np.log(float(self.max_scale)) / (half_d_feature - 1))\n",
        "        )\n",
        "        pe[:, :half_d_feature] = torch.sin(positions * div_term)\n",
        "        pe[:, half_d_feature:] = torch.cos(positions * div_term)\n",
        "        return pe[None].expand(batch_size, -1, -1)\n",
        "\n"
      ],
      "metadata": {
        "id": "kERcBAcJhf1T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#어텐션"
      ],
      "metadata": {
        "id": "6SOgetzulwfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->PegasusX\n",
        "class PegasusXAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        dropout: float = 0.0,\n",
        "        is_decoder: bool = False,\n",
        "        bias: bool = True,\n",
        "        is_causal: bool = False,\n",
        "        config: Optional[PegasusXConfig] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.config = config\n",
        "\n",
        "        if (self.head_dim * num_heads) != self.embed_dim:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
        "                f\" and `num_heads`: {num_heads}).\"\n",
        "            )\n",
        "        self.scaling = self.head_dim**-0.5\n",
        "        self.is_decoder = is_decoder\n",
        "        self.is_causal = is_causal\n",
        "\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        key_value_states: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
        "\n",
        "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
        "        # for the decoder\n",
        "        is_cross_attention = key_value_states is not None\n",
        "\n",
        "        bsz, tgt_len, _ = hidden_states.size()\n",
        "\n",
        "        # get query proj\n",
        "        query_states = self.q_proj(hidden_states) * self.scaling\n",
        "        # get key, value proj\n",
        "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
        "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
        "        # the provided `key_value_states` to support prefix tuning\n",
        "        if (\n",
        "            is_cross_attention\n",
        "            and past_key_value is not None\n",
        "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
        "        ):\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_states = past_key_value[0]\n",
        "            value_states = past_key_value[1]\n",
        "        elif is_cross_attention:\n",
        "            # cross_attentions\n",
        "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
        "        elif past_key_value is not None:\n",
        "            # reuse k, v, self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        else:\n",
        "            # self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_states, value_states)\n",
        "\n",
        "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
        "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
        "        key_states = key_states.reshape(*proj_shape)\n",
        "        value_states = value_states.reshape(*proj_shape)\n",
        "\n",
        "        src_len = key_states.size(1)\n",
        "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
        "\n",
        "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
        "            raise ValueError(\n",
        "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
        "                f\" {attn_weights.size()}\"\n",
        "            )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
        "                raise ValueError(\n",
        "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        if layer_head_mask is not None:\n",
        "            if layer_head_mask.size() != (self.num_heads,):\n",
        "                raise ValueError(\n",
        "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
        "                    f\" {layer_head_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if output_attentions:\n",
        "            # this operation is a bit awkward, but it's required to\n",
        "            # make sure that attn_weights keeps its gradient.\n",
        "            # In order to do so, attn_weights have to be reshaped\n",
        "            # twice and have to be reused in the following\n",
        "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights_reshaped = None\n",
        "\n",
        "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.bmm(attn_probs, value_states)\n",
        "\n",
        "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
        "                f\" {attn_output.size()}\"\n",
        "            )\n",
        "\n",
        "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2)\n",
        "\n",
        "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
        "        # partitioned across GPUs when using tensor-parallelism.\n",
        "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
        "\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights_reshaped, past_key_value\n",
        "\n"
      ],
      "metadata": {
        "id": "7WFJKerOhdab"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#글로벌 어텐션"
      ],
      "metadata": {
        "id": "1fkStaC0lt0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PegasusXGlobalLocalAttention(nn.Module):\n",
        "    \"\"\"Global + Local attention. For use with Encoder only.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        block_size: int,\n",
        "        dropout: float = 0.0,\n",
        "        is_decoder: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.block_size = block_size\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        if (self.head_dim * num_heads) != self.embed_dim:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
        "                f\" and `num_heads`: {num_heads}).\"\n",
        "            )\n",
        "        self.scaling = self.head_dim**-0.5\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        token_hidden_states: torch.Tensor,\n",
        "        global_hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n",
        "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
        "        dim = DimensionInfo(\n",
        "            batch_size=token_hidden_states.shape[0],\n",
        "            seq_len=token_hidden_states.shape[1],\n",
        "            block_size=self.block_size,\n",
        "            num_heads=self.num_heads,\n",
        "            hidden_dim=token_hidden_states.shape[2],\n",
        "            dim_per_head=self.head_dim,\n",
        "            num_blocks=token_hidden_states.shape[1] // self.block_size,\n",
        "            global_len=global_hidden_states.shape[1],\n",
        "            padded_seq_len=token_hidden_states.shape[1],\n",
        "        )\n",
        "\n",
        "        # [batch_size, num_heads, padded_seq_len, dim_per_head]\n",
        "        local_q = self._shape(\n",
        "            self.q_proj(token_hidden_states) * self.scaling,\n",
        "            seq_len=dim.padded_seq_len,\n",
        "            bsz=dim.batch_size,\n",
        "        )\n",
        "        local_k = self._shape(\n",
        "            self.k_proj(token_hidden_states),\n",
        "            seq_len=dim.padded_seq_len,\n",
        "            bsz=dim.batch_size,\n",
        "        )\n",
        "        local_v = self._shape(\n",
        "            self.v_proj(token_hidden_states),\n",
        "            seq_len=dim.padded_seq_len,\n",
        "            bsz=dim.batch_size,\n",
        "        )\n",
        "\n",
        "        # [batch_size, num_heads, global_len, dim_per_head]\n",
        "        global_q = self._shape(\n",
        "            self.q_proj(global_hidden_states) * self.scaling,\n",
        "            seq_len=dim.global_len,\n",
        "            bsz=dim.batch_size,\n",
        "        )\n",
        "        global_k = self._shape(\n",
        "            self.k_proj(global_hidden_states),\n",
        "            seq_len=dim.global_len,\n",
        "            bsz=dim.batch_size,\n",
        "        )\n",
        "        global_v = self._shape(\n",
        "            self.v_proj(global_hidden_states),\n",
        "            seq_len=dim.global_len,\n",
        "            bsz=dim.batch_size,\n",
        "        )\n",
        "\n",
        "        global_attn_output, global_attn_probs = self.compute_global_attention_representations(\n",
        "            global_q=global_q,\n",
        "            global_k=global_k,\n",
        "            global_v=global_v,\n",
        "            local_k=local_k,\n",
        "            local_v=local_v,\n",
        "            mask=attention_mask,\n",
        "            dim=dim,\n",
        "        )\n",
        "        local_attn_output, local_attn_probs = self.compute_local_attention_representations(\n",
        "            global_k=global_k,\n",
        "            global_v=global_v,\n",
        "            local_q=local_q,\n",
        "            local_k=local_k,\n",
        "            local_v=local_v,\n",
        "            mask=attention_mask,\n",
        "            dim=dim,\n",
        "        )\n",
        "\n",
        "        # [batch_size, global_len, hidden_dim]\n",
        "        global_attn_output = (\n",
        "            global_attn_output.transpose(1, 2).contiguous().view(dim.batch_size, dim.global_len, dim.hidden_dim)\n",
        "        )\n",
        "        # [batch_size, global_len, hidden_dim]\n",
        "        global_attn_output = self.out_proj(global_attn_output)\n",
        "        # [batch_size, num_heads, block_size, num_heads, dim_per_head]\n",
        "        local_attn_output = local_attn_output.permute(0, 2, 3, 1, 4).contiguous()\n",
        "        # [batch_size, padded_seq_len, hidden_dim]\n",
        "        local_attn_output = local_attn_output.view(dim.batch_size, dim.padded_seq_len, dim.hidden_dim)\n",
        "        # [batch_size, padded_seq_len, hidden_dim]\n",
        "        local_attn_output = self.out_proj(local_attn_output)\n",
        "\n",
        "        if output_attentions:\n",
        "            attn_probs = {\"global\": global_attn_probs, \"local\": local_attn_probs}\n",
        "        else:\n",
        "            attn_probs = None\n",
        "\n",
        "        return local_attn_output, global_attn_output, attn_probs\n",
        "\n",
        "    def compute_global_attention_representations(\n",
        "        self, global_q, global_k, global_v, local_k, local_v, mask, dim: DimensionInfo\n",
        "    ):\n",
        "        \"\"\"Compute attention representations for global tokens.\n",
        "\n",
        "        Global tokens will attend to both global tokens as well as all input sequence tokens. Because the input\n",
        "        sequence tokens are arranged in blocks for local attention, we unblock them and compute attention.\n",
        "\n",
        "        Args:\n",
        "            global_q (`torch.FloatTensor`) of shape [batch_size, num_heads, global_len, dim_per_head]:\n",
        "                query vectors from global tokens\n",
        "            global_k (`torch.FloatTensor`) of shape [batch_size, num_heads, global_len, dim_per_head]:\n",
        "                key vectors from global tokens\n",
        "            global_v (`torch.FloatTensor`) of shape [batch_size, num_heads, global_len, dim_per_head]:\n",
        "                value vectors from global tokens\n",
        "            local_k (`torch.FloatTensor`) of shape [batch_size, num_heads, padded_seq_len, dim_per_head]:\n",
        "                key vectors from local tokens\n",
        "            local_v (`torch.FloatTensor`) of shape [batch_size, num_heads, padded_seq_len, dim_per_head]:\n",
        "                value vectors from local tokens\n",
        "            mask (`torch.FloatTensor`) of shape [batch_size, padded_seq_len]: attention mask\n",
        "            dim (DimensionInfo): DimensionInfo wrapper for dimensions\n",
        "\n",
        "        Returns:\n",
        "            output of shape `[batch_sizes, length, features]`. where length will be padded to a multiple of block_size\n",
        "        \"\"\"\n",
        "        # [batch_size, num_heads, global_len+padded_seq_len, dim_per_head]\n",
        "        global_and_local_k = torch.cat([global_k, local_k], dim=2)\n",
        "        # [batch_size, num_heads, global_len+padded_seq_len, dim_per_head]\n",
        "        global_and_local_v = torch.cat([global_v, local_v], dim=2)\n",
        "\n",
        "        # [batch_size, global_len+padded_seq_len]\n",
        "        extended_mask = nn.functional.pad(mask, pad=(dim.global_len, 0), value=0)\n",
        "\n",
        "        # [batch_size, num_heads, global_len, global_len+padded_seq_len]\n",
        "        attn_weights = torch.einsum(\"BHGF,BHXF->BHGX\", global_q, global_and_local_k)\n",
        "        attn_weights = attn_weights + extended_mask[:, None, None, :]\n",
        "        attn_probs = nn.functional.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = nn.functional.dropout(attn_probs, p=self.dropout, training=self.training)\n",
        "\n",
        "        # [batch_size, num_heads, global_len, F]\n",
        "        attn_output = torch.einsum(\"BHGX,BHXF->BHGF\", attn_probs, global_and_local_v)\n",
        "        return attn_output, attn_probs\n",
        "\n",
        "    def compute_local_attention_representations(\n",
        "        self, global_k, global_v, local_q, local_k, local_v, mask, dim: DimensionInfo\n",
        "    ):\n",
        "        \"\"\"Compute attention representations for local tokens.\n",
        "\n",
        "        Local tokens will attend to both global tokens as well as all other tokens within the same local block. Hence,\n",
        "        we need to tile and concatenate the global tokens to every local block\n",
        "\n",
        "        Args:\n",
        "            global_k (`torch.FloatTensor`) of shape [batch_size, num_heads, global_len, dim_per_head]:\n",
        "                key vectors from global tokens\n",
        "            global_v (`torch.FloatTensor`) of shape [batch_size, num_heads, global_len, dim_per_head]:\n",
        "                value vectors from global tokens\n",
        "            local_q (`torch.FloatTensor`) of shape [batch_size, num_heads, padded_seq_len, dim_per_head]:\n",
        "                query vectors from local tokens\n",
        "            local_k (`torch.FloatTensor`) of shape [batch_size, num_heads, padded_seq_len, dim_per_head]:\n",
        "                key vectors from local tokens\n",
        "            local_v (`torch.FloatTensor`) of shape [batch_size, num_heads, padded_seq_len, dim_per_head]:\n",
        "                value vectors from local tokens\n",
        "            mask (`torch.FloatTensor`) of shape [batch_size, padded_seq_len]: attention mask\n",
        "            dim (DimensionInfo): DimensionInfo wrapper for dimensions\n",
        "\n",
        "        Returns:\n",
        "            output of shape `[batch_sizes, length, features]`. where length will be padded to a multiple of block_size\n",
        "        \"\"\"\n",
        "        # [batch_size, num_heads, num_blocks, block_size, dim_per_head]\n",
        "        blocked_local_q = local_q.view(dim.batch_size, dim.num_heads, dim.num_blocks, dim.block_size, dim.dim_per_head)\n",
        "        # [batch_size, num_heads, num_blocks, block_size, dim_per_head]\n",
        "        blocked_local_k = local_k.view(dim.batch_size, dim.num_heads, dim.num_blocks, dim.block_size, dim.dim_per_head)\n",
        "        # [batch_size, num_heads, num_blocks, block_size, dim_per_head]\n",
        "        blocked_local_v = local_v.view(dim.batch_size, dim.num_heads, dim.num_blocks, dim.block_size, dim.dim_per_head)\n",
        "\n",
        "        # [batch_size, num_blocks, global_len+block_size]\n",
        "        extended_mask = nn.functional.pad(\n",
        "            mask.view(dim.batch_size, dim.num_blocks, dim.block_size),\n",
        "            pad=(dim.global_len, 0),\n",
        "            value=0,\n",
        "        )\n",
        "\n",
        "        # [batch_size, num_heads, num_blocks, block_size, global_len]\n",
        "        blocked_local2global = torch.einsum(\"BHNKF,BHGF->BHNKG\", blocked_local_q, global_k)\n",
        "        # [batch_size, num_heads, num_blocks, block_size, block_size]\n",
        "        blocked_local2local = torch.einsum(\"BHNKF,BHNXF->BHNKX\", blocked_local_q, blocked_local_k)\n",
        "\n",
        "        # [batch_size, num_heads, num_blocks, block_size, global_len+block_size]\n",
        "        attn_weights = torch.cat([blocked_local2global, blocked_local2local], dim=-1)\n",
        "        attn_weights = attn_weights + extended_mask[:, None, :, None, :]\n",
        "        attn_probs = nn.functional.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = nn.functional.dropout(attn_probs, p=self.dropout, training=self.training)\n",
        "\n",
        "        # [batch_size, num_heads, num_blocks, block_size, global_len]\n",
        "        local2global_attn_probs = attn_probs[:, :, :, :, : dim.global_len]\n",
        "        # [batch_size, num_heads, num_blocks, block_size, block_size]\n",
        "        local2local_attn_probs = attn_probs[:, :, :, :, dim.global_len :]\n",
        "\n",
        "        # [batch_size, num_heads, num_blocks, block_size, dim_per_head]\n",
        "        local2global_attn_output = torch.einsum(\"BHNKG,BHGF->BHNKF\", local2global_attn_probs, global_v)\n",
        "        # [batch_size, num_heads, num_blocks, block_size, dim_per_head]\n",
        "        local2local_attn_output = torch.einsum(\"BHNKX,BHNXF->BHNKF\", local2local_attn_probs, blocked_local_v)\n",
        "        # [batch_size, num_heads, num_blocks, block_size, dim_per_head]\n",
        "        attn_output = local2global_attn_output + local2local_attn_output\n",
        "        return attn_output, attn_probs\n",
        "\n"
      ],
      "metadata": {
        "id": "u3zQ1hPjhZF7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#인코더\n"
      ],
      "metadata": {
        "id": "gps_AH-hlqbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PegasusXEncoderLayer(nn.Module):\n",
        "    def __init__(self, stagger_blocks_this_layer: bool, config: PegasusXConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.self_attn = PegasusXGlobalLocalAttention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.encoder_attention_heads,\n",
        "            block_size=config.block_size,\n",
        "            dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.global_self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.stagger_blocks_this_layer = stagger_blocks_this_layer\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        global_hidden_states: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (`torch.FloatTensor`): input to the layer of shape *(seq_len, batch, embed_dim)*\n",
        "            global_hidden_states (`torch.FloatTensor`): global token hidden states\n",
        "                *(seq_len, num_global_tokens, embed_dim)*\n",
        "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
        "                *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "        global_residual = global_hidden_states\n",
        "\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "        global_hidden_states = self.global_self_attn_layer_norm(global_hidden_states)\n",
        "\n",
        "        if self.stagger_blocks_this_layer:\n",
        "            # Pad the blocks to simulate staggering\n",
        "            hidden_states, attention_mask = self.pad_local_tokens(\n",
        "                hidden_states=hidden_states, attention_mask=attention_mask, block_size=self.block_size\n",
        "            )\n",
        "\n",
        "        hidden_states, global_hidden_states, attn_weights = self.self_attn(\n",
        "            token_hidden_states=hidden_states,\n",
        "            global_hidden_states=global_hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "\n",
        "        if self.stagger_blocks_this_layer:\n",
        "            # Undo the padding\n",
        "            hidden_states = self.unpad_local_tokens(padded_hidden_states=hidden_states, block_size=self.block_size)\n",
        "\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        global_hidden_states = nn.functional.dropout(global_hidden_states, p=self.dropout, training=self.training)\n",
        "        global_hidden_states = global_residual + global_hidden_states\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        global_residual = global_hidden_states\n",
        "        global_hidden_states = self.final_layer_norm(global_hidden_states)\n",
        "        global_hidden_states = self.activation_fn(self.fc1(global_hidden_states))\n",
        "        global_hidden_states = nn.functional.dropout(\n",
        "            global_hidden_states, p=self.activation_dropout, training=self.training\n",
        "        )\n",
        "        global_hidden_states = self.fc2(global_hidden_states)\n",
        "        global_hidden_states = nn.functional.dropout(global_hidden_states, p=self.dropout, training=self.training)\n",
        "        global_hidden_states = global_residual + global_hidden_states\n",
        "        outputs = (hidden_states, global_hidden_states)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    @classmethod\n",
        "    def pad_local_tokens(cls, hidden_states, attention_mask, block_size):\n",
        "        # hidden_states: [batch_size, seq_len, hidden_dim]\n",
        "        pad_size = block_size // 2\n",
        "        mask_min_value = torch.finfo(hidden_states.dtype).min\n",
        "        padded_hidden_states = torch.nn.functional.pad(\n",
        "            hidden_states,\n",
        "            pad=(0, 0, pad_size, pad_size),\n",
        "        )\n",
        "        padded_mask = torch.nn.functional.pad(\n",
        "            attention_mask,\n",
        "            pad=(pad_size, pad_size),\n",
        "            value=mask_min_value,\n",
        "        )\n",
        "        return padded_hidden_states, padded_mask\n",
        "\n",
        "    @classmethod\n",
        "    def unpad_local_tokens(cls, padded_hidden_states, block_size):\n",
        "        # padded_hidden_states: [batch_size, padded seq_len, hidden_dim]\n",
        "        pad_size = block_size // 2\n",
        "        return padded_hidden_states[:, pad_size:-pad_size, :]\n",
        "\n"
      ],
      "metadata": {
        "id": "rdTGx4pIhUYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#디코더"
      ],
      "metadata": {
        "id": "Cf_Sj0SxmIzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PegasusXDecoderLayer(nn.Module):\n",
        "    def __init__(self, config: PegasusXConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "\n",
        "        self.self_attn = PegasusXAttention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.encoder_attn = PegasusXAttention(\n",
        "            self.embed_dim,\n",
        "            config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = True,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (`torch.FloatTensor`): input to the layer of shape *(seq_len, batch, embed_dim)*\n",
        "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
        "                *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n",
        "            encoder_hidden_states (`torch.FloatTensor`):\n",
        "                cross attention input to the layer of shape *(seq_len, batch, embed_dim)*\n",
        "            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n",
        "                *(batch, 1, tgt_len, src_len)* where padding elements are indicated by very large negative values.\n",
        "            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "            use_cache: Whether to us KV cache for decoding\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        # Self Attention\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        # Cross-Attention Block\n",
        "        cross_attn_present_key_value = None\n",
        "        cross_attn_weights = None\n",
        "        if encoder_hidden_states is not None:\n",
        "            residual = hidden_states\n",
        "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
        "                hidden_states=hidden_states,\n",
        "                key_value_states=encoder_hidden_states,\n",
        "                attention_mask=encoder_attention_mask,\n",
        "                past_key_value=cross_attn_past_key_value,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "            hidden_states = residual + hidden_states\n",
        "\n",
        "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights, cross_attn_weights)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "NqAwBzykhQ_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#사전 모델 로딩?"
      ],
      "metadata": {
        "id": "9zi0dZMQmK3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PegasusXPreTrainedModel(PreTrainedModel):\n",
        "    config_class = PegasusXConfig\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _no_split_modules = [r\"PegasusXEncoderLayer\", r\"PegasusXDecoderLayer\"]\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.init_std\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "\n"
      ],
      "metadata": {
        "id": "sDqcrcVEhNr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#사전모델에 대한 설명"
      ],
      "metadata": {
        "id": "rNPobn_NmnXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PEGASUS_X_START_DOCSTRING = r\"\"\"\n",
        "    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
        "    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
        "    etc.)\n",
        "\n",
        "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
        "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
        "    and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config ([`PegasusXConfig`]):\n",
        "            Model configuration class with all the parameters of the model. Initializing with a config file does not\n",
        "            load the weights associated with the model, only the configuration. Check out the\n",
        "            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
        "\"\"\"\n",
        "\n",
        "PEGASUS_X_GENERATION_EXAMPLE = r\"\"\"\n",
        "    Summarization example:\n",
        "\n",
        "    ```python\n",
        "    >>> from transformers import AutoTokenizer, PegasusXForConditionalGeneration\n",
        "\n",
        "    >>> model = PegasusXForConditionalGeneration.from_pretrained(\"google/pegasus-x-base\")\n",
        "    >>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-x-large\")\n",
        "\n",
        "    >>> ARTICLE_TO_SUMMARIZE = (\n",
        "    ...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
        "    ...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
        "    ...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
        "    ... )\n",
        "    >>> inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"pt\")\n",
        "\n",
        "    >>> # Generate Summary\n",
        "    >>> summary_ids = model.generate(inputs[\"input_ids\"])\n",
        "    >>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    \"California's largest electricity provider has turned off power to hundreds of thousands of customers.\"\n",
        "    ```\n",
        "\"\"\"\n",
        "\n",
        "PEGASUS_X_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
        "            it.\n",
        "\n",
        "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "            [What are input IDs?](../glossary#input-ids)\n",
        "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
        "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            [What are attention masks?](../glossary#attention-mask)\n",
        "        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "            Indices of decoder input sequence tokens in the vocabulary.\n",
        "\n",
        "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "            [What are decoder input IDs?](../glossary#decoder-input-ids)\n",
        "\n",
        "            PEGASUS-X uses the `pad_token_id` as the starting token for `decoder_input_ids` generation. If\n",
        "            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n",
        "            `past_key_values`).\n",
        "        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n",
        "            be used by default.\n",
        "\n",
        "        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n",
        "            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n",
        "            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n",
        "            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
        "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
        "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
        "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
        "\n",
        "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
        "            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "\n",
        "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
        "            This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
        "            than the model's internal embedding lookup matrix.\n",
        "        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n",
        "            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n",
        "            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n",
        "            input (see `past_key_values`). This is useful if you want more control over how to convert\n",
        "            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
        "\n",
        "            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n",
        "            of `inputs_embeds`.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "            `past_key_values`).\n",
        "        output_attentions (`bool`, *optional*):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (`bool`, *optional*):\n",
        "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (`bool`, *optional*):\n",
        "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "sGDFFCpchLvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#사전 학습된 인코더 불러옴\n"
      ],
      "metadata": {
        "id": "wgbX0kuJmQxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PegasusXEncoder(PegasusXPreTrainedModel):\n",
        "\n",
        "\n",
        "    def __init__(self, config: PegasusXConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "        embed_dim = config.d_model\n",
        "        padding_idx = config.pad_token_id\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "        embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = PegasusXScaledWordEmbedding(\n",
        "                config.vocab_size, embed_dim, padding_idx, embed_scale=embed_scale\n",
        "            )\n",
        "\n",
        "        self.embed_global = nn.Embedding(config.num_global_tokens, embed_dim)\n",
        "        self.embed_positions = PegasusXSinusoidalPositionalEmbedding(embed_dim)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                PegasusXEncoderLayer(\n",
        "                    stagger_blocks_this_layer=i % 2 == 1 and config.stagger_local_blocks, config=config\n",
        "                )\n",
        "                for i in range(config.encoder_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(config.d_model)\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
        "\n",
        "        logger.info(f\"Setting `config.max_position_embeddings={new_num_position_embeddings}`...\")\n",
        "        self.config.max_position_embeddings = new_num_position_embeddings\n",
        "\n",
        "        self.embed_positions = PegasusXSinusoidalPositionalEmbedding(self.config.d_model)\n",
        "        self.embed_positions.to(self.device)\n",
        "\n",
        "    def get_position_embeddings(self) -> nn.Embedding:\n",
        "        \"\"\"\n",
        "        Returns the position embeddings matrix\n",
        "        \"\"\"\n",
        "        return self.embed_positions\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids)\n",
        "\n",
        "        embed_pos = self.embed_positions(inputs_embeds)\n",
        "\n",
        "        hidden_states = inputs_embeds + embed_pos\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "        # Setup mask\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(*input_shape, dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n",
        "        attention_mask = attention_mask.to(dtype=hidden_states.dtype)\n",
        "        mask_min_value = torch.finfo(hidden_states.dtype).min\n",
        "        inverted_mask = 1.0 - attention_mask\n",
        "        attention_mask = inverted_mask.masked_fill(\n",
        "            inverted_mask.to(torch.bool),\n",
        "            mask_min_value,\n",
        "        )\n",
        "\n",
        "        # padding to block_size\n",
        "        if seq_len % self.config.block_size != 0:\n",
        "            pad_len = self.config.block_size - seq_len % self.config.block_size\n",
        "            hidden_states = nn.functional.pad(hidden_states, pad=(0, 0, 0, pad_len), value=0)\n",
        "            attention_mask = nn.functional.pad(attention_mask, pad=(0, pad_len), value=mask_min_value)\n",
        "\n",
        "        # Global tokens\n",
        "        global_hidden_states = self.embed_global(\n",
        "            torch.arange(self.config.num_global_tokens, device=hidden_states.device)[None].expand(batch_size, -1)\n",
        "        )\n",
        "\n",
        "        encoder_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        for idx, encoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            to_drop = False\n",
        "            if self.training:\n",
        "                dropout_probability = torch.rand([])\n",
        "                if dropout_probability < self.layerdrop:  # skip the layer\n",
        "                    to_drop = True\n",
        "\n",
        "            if to_drop:\n",
        "                layer_outputs = (None, None)\n",
        "            else:\n",
        "                if self.gradient_checkpointing and self.training:\n",
        "                    layer_outputs = self._gradient_checkpointing_func(\n",
        "                        encoder_layer.__call__,\n",
        "                        hidden_states,\n",
        "                        global_hidden_states,\n",
        "                        attention_mask,\n",
        "                        output_attentions,\n",
        "                    )\n",
        "                else:\n",
        "                    layer_outputs = encoder_layer(\n",
        "                        hidden_states,\n",
        "                        global_hidden_states,\n",
        "                        attention_mask,\n",
        "                        output_attentions=output_attentions,\n",
        "                    )\n",
        "\n",
        "                hidden_states = layer_outputs[0]\n",
        "                global_hidden_states = layer_outputs[1]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[2],)\n",
        "\n",
        "        # Undo padding-to-block-size\n",
        "        hidden_states = hidden_states[:, :seq_len]\n",
        "\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            encoder_states = encoder_states + ((hidden_states, global_hidden_states),)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "AxIwL2QLhD2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#사전 학습된 디코더 불러옴\n"
      ],
      "metadata": {
        "id": "rxuH_7hwmrvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PegasusXDecoder(PegasusXPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`PegasusDecoderLayer`]\n",
        "\n",
        "    Args:\n",
        "        config: PegasusXConfig\n",
        "        embed_tokens (nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: PegasusXConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.decoder_layerdrop\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
        "        padding_idx = config.pad_token_id\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = PegasusXScaledWordEmbedding(\n",
        "                config.vocab_size, config.d_model, padding_idx=padding_idx, embed_scale=embed_scale\n",
        "            )\n",
        "\n",
        "        self.embed_positions = PegasusXSinusoidalPositionalEmbedding(config.d_model)\n",
        "        self.layers = nn.ModuleList([PegasusXDecoderLayer(config) for _ in range(config.decoder_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(config.d_model)\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids)\n",
        "\n",
        "        attention_mask = _prepare_4d_causal_attention_mask(\n",
        "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
        "        )\n",
        "\n",
        "        # expand encoder attention mask\n",
        "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            encoder_attention_mask = _prepare_4d_attention_mask(\n",
        "                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n",
        "            )\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(inputs_embeds, past_key_values_length)\n",
        "\n",
        "        positions = positions.to(inputs_embeds.device)\n",
        "\n",
        "        hidden_states = inputs_embeds + positions\n",
        "\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        if self.gradient_checkpointing and self.training:\n",
        "            if use_cache:\n",
        "                logger.warning_once(\n",
        "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                )\n",
        "                use_cache = False\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attns = () if output_attentions else None\n",
        "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "            if self.training:\n",
        "                dropout_probability = torch.rand([])\n",
        "                if dropout_probability < self.layerdrop:\n",
        "                    continue\n",
        "\n",
        "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "                layer_outputs = self._gradient_checkpointing_func(\n",
        "                    decoder_layer.__call__,\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    None,\n",
        "                    output_attentions,\n",
        "                    use_cache,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=attention_mask,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    encoder_attention_mask=encoder_attention_mask,\n",
        "                    past_key_value=past_key_value,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attns += (layer_outputs[1],)\n",
        "\n",
        "                if encoder_hidden_states is not None:\n",
        "                    all_cross_attentions += (layer_outputs[2],)\n",
        "\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "\n",
        "        # add hidden states from the last decoder layer\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        next_cache = next_decoder_cache if use_cache else None\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attns,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "zo2iobe2g-zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#모델 선언(전체)"
      ],
      "metadata": {
        "id": "W20MgujoohTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PegasusXModel(PegasusXPreTrainedModel):\n",
        "    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n",
        "\n",
        "    def __init__(self, config: PegasusXConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        vocab_size = config.vocab_size\n",
        "        embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
        "        padding_idx = config.pad_token_id\n",
        "        self.shared = PegasusXScaledWordEmbedding(\n",
        "            vocab_size, config.d_model, padding_idx=padding_idx, embed_scale=embed_scale\n",
        "        )\n",
        "\n",
        "        self.encoder = PegasusXEncoder(config, self.shared)\n",
        "        self.decoder = PegasusXDecoder(config, self.shared)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.decoder\n",
        "\n",
        "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
        "        \"\"\"\n",
        "        Resizes position embeddings matrix of the model if `new_num_position_embeddings !=\n",
        "        config.max_position_embeddings`.\n",
        "\n",
        "        Arguments:\n",
        "            new_num_position_embeddings (`int`):\n",
        "                The number of new position embeddings. If position embeddings are learned, increasing the size will add\n",
        "                newly initialized vectors at the end, whereas reducing the size will remove vectors from the end. If\n",
        "                position embeddings are not learned (*e.g.* sinusoidal position embeddings), increasing the size will\n",
        "                add correct vectors at the end following the position encoding algorithm, whereas reducing the size\n",
        "                will remove vectors from the end.\n",
        "        \"\"\"\n",
        "        self.config.max_position_embeddings = new_num_position_embeddings\n",
        "        self.encoder.resize_position_embeddings(new_num_position_embeddings)\n",
        "        self.decoder.resize_position_embeddings(new_num_position_embeddings)\n",
        "\n",
        "    def get_position_embeddings(self) -> Tuple[nn.Embedding]:\n",
        "        \"\"\"\n",
        "        Returns the position embeddings matrix\n",
        "        \"\"\"\n",
        "        return (self.encoder.get_position_embeddings(), self.decoder.get_position_embeddings())\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(PEGASUS_X_INPUTS_DOCSTRING)\n",
        "    @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_input_ids: Optional[torch.Tensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n",
        "        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, Seq2SeqModelOutput]:\n",
        "\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_mask,\n",
        "            encoder_hidden_states=encoder_outputs[0],\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "Fv4OFKrDg51c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#조건 부여한 모델 생성(전체)"
      ],
      "metadata": {
        "id": "lXPk12p5oomh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@add_start_docstrings(\"The PEGASUS-X for conditional generation (e.g. summarization).\", PEGASUS_X_START_DOCSTRING)\n",
        "class PegasusXForConditionalGeneration(PegasusXPreTrainedModel):\n",
        "    base_model_prefix = \"model\"\n",
        "    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n",
        "\n",
        "    def __init__(self, config: PegasusXConfig):\n",
        "        super().__init__(config)\n",
        "        self.model = PegasusXModel(config)\n",
        "        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.model.get_encoder()\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model.get_decoder()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
        "\n",
        "        self.config.max_position_embeddings = new_num_position_embeddings\n",
        "        self.model.encoder.resize_position_embeddings(new_num_position_embeddings)\n",
        "        self.model.decoder.resize_position_embeddings(new_num_position_embeddings)\n",
        "\n",
        "    def get_position_embeddings(self) -> Tuple[nn.Embedding]:\n",
        "        \"\"\"\n",
        "        Returns the position embeddings matrix\n",
        "        \"\"\"\n",
        "        return (self.model.encoder.get_position_embeddings(), self.model.decoder.get_position_embeddings())\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(PEGASUS_X_INPUTS_DOCSTRING)\n",
        "    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    @add_end_docstrings(PEGASUS_X_GENERATION_EXAMPLE)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_input_ids: Optional[torch.Tensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n",
        "        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, Seq2SeqLMOutput]:\n",
        "\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if labels is not None:\n",
        "            if use_cache:\n",
        "                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n",
        "            use_cache = False\n",
        "            if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "                decoder_input_ids = shift_tokens_right(\n",
        "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "                )\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        lm_logits = self.lm_head(outputs[0])\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + outputs[1:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=lm_logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions=outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        decoder_input_ids,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        use_cache=None,\n",
        "        encoder_outputs=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past_key_values is not None:\n",
        "            past_length = past_key_values[0][0].shape[2]\n",
        "\n",
        "            # Some generation methods already pass only the last input ID\n",
        "            if decoder_input_ids.shape[1] > past_length:\n",
        "                remove_prefix_length = past_length\n",
        "            else:\n",
        "                # Default to old behavior: keep only final ID\n",
        "                remove_prefix_length = decoder_input_ids.shape[1] - 1\n",
        "\n",
        "            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"past_key_values\": past_key_values,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
        "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past_key_values, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past_key_values:\n",
        "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n",
        "                + layer_past[2:],\n",
        "            )\n",
        "        return reordered_past\n",
        "\n"
      ],
      "metadata": {
        "id": "wxQWun8Tg11G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->PegasusX\n",
        "class PegasusXDecoderWrapper(PegasusXPreTrainedModel):\n",
        "    \"\"\"\n",
        "    This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is\n",
        "    used in combination with the [`EncoderDecoderModel`] framework.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.decoder = PegasusXDecoder(config)\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.decoder(*args, **kwargs)"
      ],
      "metadata": {
        "id": "MFImzoqGgndt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}