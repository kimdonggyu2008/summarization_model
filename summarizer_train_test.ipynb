{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "m1CyAMxCgEqj"
      ],
      "authorship_tag": "ABX9TyNNaRSUc8NfGaSYJj/rpl4F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/summarization_model/blob/main/summarizer_train_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ìš”ì•½ ëª¨ë¸(pegasus)í•™ìŠµ ë° ì¶”ë¡  ì½”ë“œ"
      ],
      "metadata": {
        "id": "Em1hW4fSwzH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "58aw0GJxUjm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14c100b-3fc4-4aeb-a188-c0ac795b33b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "GI-FEIIuUlRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b31b8e0-e15a-4e39-e326-b1bcdbbacaef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Iiq0LkzUCUT"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ë°ì´í„° ë³‘í•© ë° í† í¬ë‚˜ì´ì €, ì²˜ë¦¬(í•„ìš”ì‹œ ì‚¬ìš©)\n"
      ],
      "metadata": {
        "id": "m1CyAMxCgEqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ë³‘í•©\n",
        "\n",
        "# # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "# # ë°ì´í„° ë¡œë“œ ë° ë³‘í•©\n",
        "# data_folder_path = \"/content/drive/MyDrive/summarizer/data\"\n",
        "# dataframes = []\n",
        "\n",
        "# for filename in os.listdir(data_folder_path):\n",
        "#     if filename.endswith(\".csv\"):\n",
        "#         file_path = os.path.join(data_folder_path, filename)\n",
        "#         df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "#         # íŠ¹ì • ì—´ ì´ë¦„ë§Œ ë°”ê¾¸ê¸°\n",
        "#         if filename == \"news_summary.csv\":\n",
        "#             df = df.rename(columns={\n",
        "#                 \"ctext\": \"article\",  # 'ctext'ë¥¼ 'article'ë¡œ ë³€ê²½\n",
        "#                 \"text\": \"highlights\"  # 'text'ë¥¼ 'highlights'ë¡œ ë³€ê²½\n",
        "#             })\n",
        "\n",
        "#         elif filename == \"news_summary_more.csv\":\n",
        "#             df = df.rename(columns={\n",
        "#                 \"headlines\": \"highlights\",  # 'headlines'ë¥¼ 'highlights'ë¡œ ë³€ê²½\n",
        "#                 \"text\": \"article\"  # 'text'ë¥¼ 'article'ë¡œ ë³€ê²½\n",
        "#             })\n",
        "\n",
        "#         # ë°ì´í„°í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
        "#         dataframes.append(df)\n",
        "\n",
        "# # ë°ì´í„°í”„ë ˆì„ ë³‘í•©\n",
        "# combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# # NaNì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´\n",
        "# combined_df['article'] = combined_df['article'].fillna(\"\")\n",
        "\n",
        "# # 'article' ì—´ì˜ ê°’ì´ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³ , ë¬¸ìì—´ì´ ì•„ë‹Œ ê°’ì€ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´\n",
        "# combined_df['article'] = combined_df['article'].apply(lambda x: str(x) if isinstance(x, str) else \"\")\n",
        "\n",
        "# # ë¹ˆ í–‰(ë¹ˆ ë¬¸ìì—´ í¬í•¨)ì„ ì‚­ì œ\n",
        "# combined_df = combined_df[combined_df['article'].str.strip() != \"\"]\n",
        "\n",
        "# combined_df.to_csv(\"/content/drive/MyDrive/summarizer/data/combined_dataset.csv\", index=False)\n",
        "\n",
        "\n",
        "# dataset = Dataset.from_pandas(combined_df)\n",
        "\n",
        "# train_test = dataset.train_test_split(test_size=0.1)\n",
        "# datasets = DatasetDict({\n",
        "#     'train': train_test['train'],\n",
        "#     'test': train_test['test']\n",
        "# })\n",
        "\n"
      ],
      "metadata": {
        "id": "WyPqSis3P0Mq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/summarizer/data/combined_dataset.csv')\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# # Pegasus ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì¤€ë¹„\n",
        "# model_name = \"google/pegasus-xsum\"\n",
        "# tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "# model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# # Pandas DataFrameì„ Hugging Face Datasetìœ¼ë¡œ ë³€í™˜\n",
        "# combined_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# # ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ (GPUì—ì„œ ì…ë ¥ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ë³€ê²½)\n",
        "# def preprocess_function(examples):\n",
        "#     inputs = tokenizer(examples['article'], padding=\"max_length\", truncation=True)\n",
        "#     labels = tokenizer(examples['highlights'], padding=\"max_length\", truncation=True)\n",
        "#     inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return inputs\n",
        "\n",
        "# # 1. Hugging Face Datasetsë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ í† í¬ë‚˜ì´ì§• (ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬)\n",
        "# print(\"Tokenizing the dataset...\")\n",
        "# tokenized_datasets = combined_dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "23-CzDW8O1nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ì…‹ì„ 8ê°œë¡œ ë‚˜ëˆ”\n",
        "# dataset_length = len(tokenized_datasets)\n",
        "# split_size = dataset_length // 8  # ë°ì´í„°ë¥¼ 8ë“±ë¶„\n",
        "\n",
        "# # 3. ê° íŒŒíŠ¸ë¥¼ pkl íŒŒì¼ë¡œ ì €ì¥í•  ê²½ë¡œ ì„¤ì •\n",
        "# output_dir = \"/content/drive/MyDrive/summarizer/preprocessed/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# for i in range(8):\n",
        "#     start_idx = i * split_size\n",
        "#     # ë§ˆì§€ë§‰ íŒŒíŠ¸ëŠ” ë‚¨ì€ ëª¨ë“  ë°ì´í„°ë¥¼ í¬í•¨\n",
        "#     end_idx = (i + 1) * split_size if i < 7 else dataset_length  # ë§ˆì§€ë§‰ íŒŒíŠ¸ëŠ” ì „ì²´ ëê¹Œì§€ í¬í•¨\n",
        "\n",
        "#     # ê° íŒŒíŠ¸ì˜ ë°ì´í„° ì¶”ì¶œ (Hugging Face Datasetì—ì„œ select ì‚¬ìš©)\n",
        "#     part_dataset = tokenized_datasets.select(range(start_idx, end_idx))\n",
        "\n",
        "#     # pickle íŒŒì¼ë¡œ ì €ì¥\n",
        "#     output_file = os.path.join(output_dir, f\"processed_dataset_part_{i+1}.pkl\")\n",
        "#     with open(output_file, 'wb') as f:\n",
        "#         pickle.dump(part_dataset, f)\n",
        "\n",
        "#     print(f\"Tokenized dataset part {i+1} saved to {output_file}\")"
      ],
      "metadata": {
        "id": "fSGekbkW9L5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#í•™ìŠµ ì„¸íŒ…(í† í¬ë‚˜ì´ì§• ëœ ë°ì´í„° ë°›ìŒ)\n"
      ],
      "metadata": {
        "id": "gtMc2iUKgKSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ì„¤ì •, ì²˜ìŒ í•™ìŠµ ì‹œí‚¬ë•Œ\n",
        "# model_name = \"google/pegasus-xsum\"\n",
        "# tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "# model = PegasusForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "oA2DD-2SU5sh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ì„¤ì •, ì§ì ‘ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ê°€ì ¸ì™€ í•™ìŠµì‹œí‚¬ë•Œ\n",
        "model_name = \"/content/drive/MyDrive/summarizer/model/model_part_1\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "ei_d0jc5GDDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì • (5 ì—í­ì”© í•™ìŠµ)\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=f\"/content/drive/MyDrive/summarizer/checkpoints\",\n",
        "        evaluation_strategy=\"epoch\",  # ë§¤ ì—í¬í¬ë§ˆë‹¤ í‰ê°€\n",
        "        save_strategy=\"epoch\",  # ë§¤ ì—í¬í¬ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "        save_steps=1000,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=4,  # ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
        "        per_device_eval_batch_size=4,    # ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
        "        num_train_epochs=5,  # ê°ê° 5 ì—í­ì”© í•™ìŠµ\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"/content/drive/MyDrive/summarizer/logging\",\n",
        "        logging_steps=1000,\n",
        "        save_total_limit=2,  # ìµœëŒ€ 2ê°œì˜ ì²´í¬í¬ì¸íŠ¸ë§Œ ì €ì¥\n",
        "        load_best_model_at_end=True,  # ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ ë¡œë“œ\n",
        "        metric_for_best_model=\"eval_loss\",  # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì„ ì„ íƒí•  ê¸°ì¤€\n",
        "        greater_is_better=True,  # ë‚®ì€ eval_lossê°€ ë” ì¢‹ì€ ëª¨ë¸ì„ì„ ë‚˜íƒ€ëƒ„\n",
        "    )"
      ],
      "metadata": {
        "id": "UdnRFB05HE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42850257-3c4b-49fe-c9aa-1e363fae66fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint_path = \"/content/drive/MyDrive/summarizer/checkpoints\""
      ],
      "metadata": {
        "id": "fh3QxRgZgK3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #ëª¨ë¸ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°, ì²´í¬í¬ì¸íŠ¸ ì•ˆì¨ë„ ë¨, ì—†ì„ë•Œë§Œ ì“°ì„¸ìš”\n",
        "# last_checkpoint = None\n",
        "# if os.path.isdir(model_checkpoint_path) and os.listdir(model_checkpoint_path):\n",
        "#     last_checkpoint = Trainer.get_last_checkpoint(model_checkpoint_path)\n",
        "#     if last_checkpoint is not None:\n",
        "#         print(f\"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤: {last_checkpoint}\")\n",
        "#     else:\n",
        "#         print(\"ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
        "# else:\n",
        "#     print(\"ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n"
      ],
      "metadata": {
        "id": "niR216SpNfAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#í•™ìŠµ(í•™ìŠµ íŒŒì¼ì€ ì—¬ëŸ¬ê°œë¡œ ë˜ì–´ìˆìŒ)"
      ],
      "metadata": {
        "id": "FNrf6O3BHUL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸, í•™ìŠµì´ í•˜ë‚˜ ëë‚ ë•Œë§ˆë‹¤ í•œê°œì”© ì£¼ì„ì²˜ë¦¬í•˜ê¸°\n",
        "file_paths = [\n",
        "    #\"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/summarizer/processed_dataset_part_1.pkl\",\n",
        "    \"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/summarizer/processed_dataset_part_2.pkl\",\n",
        "    \"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/summarizer/processed_dataset_part_3.pkl\",\n",
        "    \"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/summarizer/processed_dataset_part_4.pkl\",\n",
        "    \"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/summarizer/processed_dataset_part_5.pkl\",\n",
        "    \"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/summarizer/processed_dataset_part_6.pkl\",\n",
        "    \"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/summarizer/processed_dataset_part_7.pkl\",\n",
        "    \"/content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/summarizer/processed_dataset_part_8.pkl\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "tq_RRb46-Cat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_preprocessed_data(file_path, split_ratio=0.8):\n",
        "#     with open(file_path, 'rb') as f:\n",
        "#         tokenized_datasets = pickle.load(f)\n",
        "\n",
        "#     # í•™ìŠµ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë‚˜ëˆ” (default: 80% train, 20% eval)\n",
        "#     train_size = int(split_ratio * len(tokenized_datasets))\n",
        "#     train_dataset = tokenized_datasets[:train_size]\n",
        "#     eval_dataset = tokenized_datasets[train_size:]\n",
        "\n",
        "#     return train_dataset, eval_dataset\n"
      ],
      "metadata": {
        "id": "37S-jjJ-mcYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        tokenized_datasets = pickle.load(f)\n",
        "    return tokenized_datasets"
      ],
      "metadata": {
        "id": "uolZLlAd1mP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ê° íŒŒì¼ ê°€ì§€ê³  í•™ìŠµì‹œí‚´, datasetë²ˆí˜¸ëŠ” ì‹ ê²½ ì•ˆì“°ì…”ë„ ë©ë‹ˆë‹¤. file_pathì—ì„œ ì½ì–´ì˜¤ëŠ” íŒŒì¼ë§Œ ë§ìœ¼ë©´ ë¼ìš”\n",
        "for i, file_path in enumerate(file_paths):\n",
        "    print(f\"Loading dataset part {i+1} from {file_path}\")\n",
        "\n",
        "\n",
        "    loaded_data = load_preprocessed_data(file_path)\n",
        "    #train_dataset, eval_dataset = load_preprocessed_data(file_path)\n",
        "\n",
        "    split_data = loaded_data.train_test_split(test_size=0.2)\n",
        "\n",
        "    train_dataset = split_data['train']\n",
        "    eval_dataset = split_data['test']\n",
        "\n",
        "\n",
        "    #Trainer ì„¤ì •\n",
        "    trainer = Trainer(\n",
        "        model=model,  # ì¼ë°˜ ëª¨ë¸ë¡œ ì„¤ì •\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,  # í•™ìŠµ ë°ì´í„°ì…‹\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # ì¡°ê¸° ì¢…ë£Œ: ì„±ëŠ¥ ê°œì„  ì—†ìœ¼ë©´ 3ë²ˆ í›„ ì¢…ë£Œ\n",
        "    )\n",
        "    # trainer = Trainer(\n",
        "    #     model=model,\n",
        "    #     args=training_args,\n",
        "    #     train_dataset=train_dataset,  # í•™ìŠµ ë°ì´í„°ì…‹\n",
        "    #     eval_dataset=eval_dataset,    # í‰ê°€ ë°ì´í„°ì…‹ ì¶”ê°€\n",
        "    #     data_collator=data_collator,\n",
        "    #     tokenizer=tokenizer\n",
        "    # )\n",
        "\n",
        "    # í•™ìŠµ ì‹œì‘\n",
        "    print(f\"Training on dataset part {i+1}...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"Training interrupted during part {i+1}. Last checkpoint saved.\")\n",
        "\n",
        "    # apií‚¤ëŠ” ì´ê±¸ë¡œ ë„£ìœ¼ë©´ ë¨ 513a1f0c050fa7f60a76b5232e904d8df397082e\n",
        "\n",
        "    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "    trainer.save_model(f\"/content/drive/MyDrive/summarizer/model/model_part_{i+1}\")\n",
        "    print(f\"Model checkpoint saved after part {i+1}\")\n"
      ],
      "metadata": {
        "id": "0VxrQGf5-Sj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "c6ffecb1-26dd-4bd8-bd64-4a289be747a5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset part 1 from /content/drive/MyDrive/combined/á„á…¢á†¸á„‰á…³á„á…©á†« á„ƒá…¦á„‹á…µá„á…¥ á„Œá…¥á†«á„á…¥á„…á…µ á„‘á…¡á„‹á…µá†¯/summarizer/processed_dataset_part_2.pkl\n",
            "Training on dataset part 1...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241027_043418-9gfjupfg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface/runs/9gfjupfg' target=\"_blank\">/content/drive/MyDrive/summarizer/checkpoints</a></strong> to <a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface' target=\"_blank\">https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface/runs/9gfjupfg' target=\"_blank\">https://wandb.ai/dkkim2008-hankuk-university-for-foreign-studies/huggingface/runs/9gfjupfg</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1558' max='48740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1558/48740 37:33 < 18:58:38, 0.69 it/s, Epoch 0.16/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1803' max='48740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1803/48740 43:28 < 18:52:50, 0.69 it/s, Epoch 0.18/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/summarizer/final_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/summarizer/final_model\")\n",
        "print(\"Final model and tokenizer saved.\")"
      ],
      "metadata": {
        "id": "xdhlyIkw-WLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ì…ë ¥ ë° ì˜ˆì¸¡"
      ],
      "metadata": {
        "id": "UdOMaW__glI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ì‹œì‘ ë° ìš”ì•½ ê²°ê³¼ í™•ì¸\n",
        "def summarize_article(article_text):\n",
        "    inputs = tokenizer(article_text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "    summary_ids = model.generate(**inputs, max_length=60, num_beams=5, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "ZReqxiA4bZpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‚¬ìš©ì ì…ë ¥ ë¬¸ì¥ì„ ìš”ì•½\n",
        "article_text = \"ì—¬ê¸°ë‹¤ê°€ ì…ë ¥\"\n",
        "summary = summarize_article(article_text)\n",
        "print(\"\\nSummary:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "rmwCM7XSH0mf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}